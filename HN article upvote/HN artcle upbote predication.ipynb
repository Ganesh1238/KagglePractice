{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "545d6445-440e-4b17-81ce-e5b8928dfc5a",
    "_uuid": "c4cb0fbe364f5144406d15b4ea4060acba6dc6af"
   },
   "source": [
    "[Hacker News](https://news.ycombinator.com) is a community where users can submit articles, and other users can upvote those articles. The articles with the most upvotes make it to the front page, where they're more visible to the community.\n",
    "\n",
    "Our data set consists of submissions users made to Hacker News. A few developers have gathered this data. I will use the dataset uploaded by [Anthony](https://www.kaggle.com/antgoldbloom). This data set is Hacker News posts from the last 12 months (up to September 26 2016). Our data has the following columns:\n",
    "* `title`: title of the post (self explanatory)\n",
    "* `url`: the url of the item being linked to\n",
    "* `num_points`: the number of upvotes the post received\n",
    "* `num_comments`: the number of comments the post received\n",
    "* `author`: the name of the account that made the post\n",
    "* `created_at`: the date and time the post was made (the time zone is Eastern Time in the US)\n",
    "---\n",
    "**Goal:** to train a linear regression model that predicts the number of upvotes a headline would receive. To do this, we'll need to convert each headline to a numerical representation. We'll use the **bag of words** appraoch where:\n",
    ">a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity ~Wiki\n",
    "\n",
    "The first step in creating a bag of words model is tokenization. In tokenization, we break a sentence up into disconnected words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0bd74763-ff29-4d3a-bd13-961de39d642d",
    "_uuid": "6f39ead8102f7950773c353fcbb1634df1d07832"
   },
   "source": [
    "## <center> Data Exploration <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "32f284be-224a-425b-832b-2b1aaf55e7c1",
    "_uuid": "490322d3f49cd9d912e504bfe710e9d570444e73",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset is: (293119, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>num_points</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12579008</td>\n",
       "      <td>You have two days to comment if you want stem ...</td>\n",
       "      <td>http://www.regulations.gov/document?D=FDA-2015...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>altstar</td>\n",
       "      <td>9/26/2016 3:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12579005</td>\n",
       "      <td>SQLAR  the SQLite Archiver</td>\n",
       "      <td>https://www.sqlite.org/sqlar/doc/trunk/README.md</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>blacksqr</td>\n",
       "      <td>9/26/2016 3:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12578997</td>\n",
       "      <td>What if we just printed a flatscreen televisio...</td>\n",
       "      <td>https://medium.com/vanmoof/our-secrets-out-f21...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>9/26/2016 3:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12578989</td>\n",
       "      <td>algorithmic music</td>\n",
       "      <td>http://cacm.acm.org/magazines/2011/7/109891-al...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>poindontcare</td>\n",
       "      <td>9/26/2016 3:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12578979</td>\n",
       "      <td>How the Data Vault Enables the Next-Gen Data W...</td>\n",
       "      <td>https://www.talend.com/blog/2016/05/12/talend-...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>markgainor1</td>\n",
       "      <td>9/26/2016 3:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  12579008  You have two days to comment if you want stem ...   \n",
       "1  12579005                         SQLAR  the SQLite Archiver   \n",
       "2  12578997  What if we just printed a flatscreen televisio...   \n",
       "3  12578989                                  algorithmic music   \n",
       "4  12578979  How the Data Vault Enables the Next-Gen Data W...   \n",
       "\n",
       "                                                 url  num_points  \\\n",
       "0  http://www.regulations.gov/document?D=FDA-2015...           1   \n",
       "1   https://www.sqlite.org/sqlar/doc/trunk/README.md           1   \n",
       "2  https://medium.com/vanmoof/our-secrets-out-f21...           1   \n",
       "3  http://cacm.acm.org/magazines/2011/7/109891-al...           1   \n",
       "4  https://www.talend.com/blog/2016/05/12/talend-...           1   \n",
       "\n",
       "   num_comments        author      created_at  \n",
       "0             0       altstar  9/26/2016 3:26  \n",
       "1             0      blacksqr  9/26/2016 3:24  \n",
       "2             0  pavel_lishin  9/26/2016 3:19  \n",
       "3             0  poindontcare  9/26/2016 3:16  \n",
       "4             0   markgainor1  9/26/2016 3:14  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "    \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    \n",
    "submissions = pd.read_csv('HN_posts_year_to_Sep_26_2016.csv')\n",
    "print('Shape of dataset is:',submissions.shape)\n",
    "submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e2abad88-cad2-4bf0-a131-8b33554f294f",
    "_uuid": "496e8d07f45b06d8102af9dba6d631c5acb8f8d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% Missing Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>4.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_points</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_comments</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              % Missing Values\n",
       "id                        0.00\n",
       "title                     0.00\n",
       "url                       4.73\n",
       "num_points                0.00\n",
       "num_comments              0.00\n",
       "author                    0.00\n",
       "created_at                0.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percent of missing values for each column\n",
    "pd.DataFrame(submissions.isnull().sum()/submissions.shape[0]*100,columns=['% Missing Values']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "9a78edc7-6cc4-4fd0-aecd-f6a79b0d9344",
    "_uuid": "f40f382498ec885c178f81803163193aebda308f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest number of posts is 2731 made by jonbaer\n"
     ]
    }
   ],
   "source": [
    "#who has posted the most? and how many?\n",
    "print('Highest number of posts is {1} made by {0}'.format(submissions['author'].value_counts().index.tolist()[0],submissions['author'].value_counts().tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f6d390da-38e5-46a8-b520-3fc9d438a7dc",
    "_uuid": "44dd9a9a9adf7954f8ef00d877a5f0e85dd5e995"
   },
   "source": [
    "Oh wow! such an active user! let's see how much points he has recieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "7d4d1b8c-66cf-4587-b5df-1d2af551eae8",
    "_uuid": "ffb2bee61af12ed905fc3e4a8d79196465b04de4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jonbaer recieved 9.58 average points, while average points for all posts is 15.03\n"
     ]
    }
   ],
   "source": [
    "jonbaer=submissions[submissions['author']=='jonbaer']\n",
    "print('jonbaer recieved {0:.2f} average points, while average points for all posts is {1:.2f}'.format(jonbaer['num_points'].mean(),submissions['num_points'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae1b0c35-effa-4674-8826-da41ac2abe3c",
    "_uuid": "d2321ada8cb115f914b812ffe62512b0bc26fda0"
   },
   "source": [
    "Now let's look at who has gotten the most number of votes on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "de368301-94e5-43de-b156-a49a6e68fa3c",
    "_uuid": "55ac61644d1cc70c5fc307d9377107aad5775618",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "intull         2011.0\n",
       "alankay1       1401.0\n",
       "mmebane        1248.0\n",
       "boren_ave11    1213.0\n",
       "hannahmitt     1172.0\n",
       "Name: num_points, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_votes_byauthor=submissions.groupby('author').mean()\n",
    "ave_votes_byauthor['num_points'].sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "26db3838-2142-4c61-819d-51ba3f30e794",
    "_uuid": "cd37575a58347d9508b3f31b57a226cb042eba4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "def histly(df,target):\n",
    "    title_text='Histogram of log of average {0} by user'.format(target)\n",
    "    \n",
    "    data = [go.Histogram(x=np.log1p(df[target]))]\n",
    "    \n",
    "    shapes_list=[{\n",
    "        'type': 'line',\n",
    "        'xref': 'x',\n",
    "        'yref': 'paper',\n",
    "        'x0': np.log1p(df[target].mean()),\n",
    "        'y0':0,\n",
    "        'x1': np.log1p(df[target].mean()),\n",
    "        'y1':1,\n",
    "        'line': {\n",
    "            'color': 'b',\n",
    "            'width': 5,\n",
    "            'dash': 'dashdot'\n",
    "        }}]\n",
    "        \n",
    "    annotations_list=[{\n",
    "            'x':np.log1p(df[target].mean()),\n",
    "            'y': 50,\n",
    "            'xref':'x',\n",
    "            'yref':'y',\n",
    "            'text':'Average across all data',\n",
    "            'showarrow':True,\n",
    "            'arrowhead':7,\n",
    "            'ax':100,\n",
    "            'ay':-100\n",
    "            }]\n",
    "        \n",
    "    layout = go.Layout(\n",
    "        title=title_text,\n",
    "        font=dict(size=14, color='b'),\n",
    "        xaxis={\n",
    "        'title':'Log of average',\n",
    "        'titlefont':{\n",
    "            'size':18,\n",
    "            'color':'b'\n",
    "        }\n",
    "        },\n",
    "        yaxis={\n",
    "        'title':'Count',\n",
    "        'titlefont':{\n",
    "            'size':18,\n",
    "            'color':'b'\n",
    "        }\n",
    "        },\n",
    "        autosize=True,\n",
    "        shapes=shapes_list,\n",
    "        annotations=annotations_list\n",
    "        )\n",
    "    \n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "e27d591f-691d-432f-a0f3-b617faf47d4f",
    "_uuid": "305eda74672ab8708142d447cc4917901c98eafc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "histly(ave_votes_byauthor,'num_points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96a30483-0299-4829-bab2-1779d3d71717",
    "_uuid": "248fdf6fe0baf6661c78aac80750547c52698515"
   },
   "source": [
    "## <center> Date Preparation <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d9f33a34-4e3c-453f-8cd3-cce894186b8e",
    "_uuid": "728e55292210ea3794256a78287a8c9d0f7723ea",
    "collapsed": true
   },
   "source": [
    "I dont plan to utilize url_hostname for now. The only columns I will be using for regression analysis are headlines and points. Let's only gather `title` and `num_points`, and then drop na `title` rows.\n",
    "\n",
    "**Note** due to the large size of the data, when I tried to create the `counts` dataframe, I got a Memory Error. Therefore, for the representation purposes, I only use a small portion of the data. However, I will switch to the full dataset when fitting the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "e5c981a7-acb5-4da1-bc80-1c094139c0b9",
    "_uuid": "6bc394feb49222eb3cc6441931a20825695a3d3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29312, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=submissions.loc[:,['title','num_points']]\n",
    "\n",
    "#sampling 5% of the daset for the representation purposes of the next two steps.\n",
    "train=train.sample(frac=0.10,axis=0).reset_index()\n",
    "\n",
    "train=train.dropna()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2238b028-e4cf-4a40-800e-7d6524d92630",
    "_uuid": "eb68bb4582f3b127ee47b5de07b73f430a6d2011"
   },
   "source": [
    "There are four ways to remove punctuations:\n",
    "* **sets**\n",
    "    - exclude = set(string.punctuation) \\n s = ''.join(ch for ch in s if ch not in exclude)\n",
    "* **regex**\n",
    "    - s = re.sub(r'[^\\w\\s]','',s) OR re.compile('[%s]' % re.escape(string.punctuation)).sub('',s)\n",
    "* **translate**\n",
    "    - s = s.translate(str.maketrans('','',string.punctuation))\n",
    "* **replace**\n",
    "    - for c in string.punctuation: \\n s=s.replace(c,\"\")\n",
    "\n",
    "Among all these approaches, `translate()` method beats the others in terms of speed. please refer to **[this post](https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python)** on StackOverflow. But please note that the syntax mentioned for `translate()` in that post is applicable in Python 2. For Python 3, please refer to **[this post](https://stackoverflow.com/questions/23175809/str-translate-gives-typeerror-translate-takes-one-argument-2-given-worked-i)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW CORPUS:\n",
      "--------------------\n",
      "['You have two days to comment if you want stem cells to be classified as your own', 'SQLAR  the SQLite Archiver', 'What if we just printed a flatscreen television on the side of our boxes?', 'algorithmic music', 'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake', 'Saving the Hassle of Shopping']\n"
     ]
    }
   ],
   "source": [
    "import normalization as nm\n",
    "\n",
    "# build the raw corpus\n",
    "\n",
    "corpus = nm.build_corpus(submissions, columns=['title'])\n",
    "print('\\nRAW CORPUS:')\n",
    "print('-'*20)\n",
    "print(corpus[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7aee25de8319>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# normalize the corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnorm_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nNORMALIZED CORPUS:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Personal\\Github\\Kaggle\\HN article upvote\\normalization.py\u001b[0m in \u001b[0;36mnormalize_corpus\u001b[1;34m(corpus, html_stripping, contraction_expansion, accented_char_removal, text_lower_case, text_lemmatization, special_char_removal, stopword_removal, text_stem)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtext_lemmatization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtext_stem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Personal\\Github\\Kaggle\\HN article upvote\\normalization.py\u001b[0m in \u001b[0;36mlemmatize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# # Lemmatizing text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'-PRON-'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.precompute_hiddens.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spacy\\_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         Yf = self.ops.xp.dot(X,\n\u001b[1;32m--> 148\u001b[1;33m             self.W.reshape((self.nF*self.nO*self.nP, self.nI)).T)\n\u001b[0m\u001b[0;32m    149\u001b[0m         \u001b[0mYf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mYf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# normalize the corpus\n",
    "\n",
    "norm_corpus = nm.normalize_corpus(corpus)\n",
    "print('\\nNORMALIZED CORPUS:')\n",
    "print('-'*20)\n",
    "print(norm_corpus[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['processed_text'] = norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vect, tfidf_features = fe.tfidf_vectorizer(norm_corpus, max_df=1.0, \n",
    "                                                 min_df=0.0, use_idf=True, \n",
    "                                                 ngram_range=(1,1))\n",
    "print('\\nFEATURE MATRIX SHAPE:')\n",
    "print('-'*20)\n",
    "print(tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e0082fd-2187-4637-9689-48b8466305ba",
    "_uuid": "8370b5987d2e27d97664f6a47503a8fa1e4af032",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing the punctuations.\n",
    "import string\n",
    "train['title_nopuncs']=train['title'].apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74569b3a-824e-4b6a-b818-a14cf4c6d7c1",
    "_uuid": "7ccfd842fee6952db84a21428c7ea8195d481f16"
   },
   "source": [
    "Also, we should lower case the titles. Apple, apple, and APPLE are all the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3caf4fe6-bdfe-4595-aaf1-1b17eee8d4fd",
    "_uuid": "5f0ae3f92249b8cf76edcbe9a594915865c01055",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lower casing titles\n",
    "train['title_nopuncs']=train['title_nopuncs'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8d91ba63-4c18-417b-9bd1-2ef6cd078d59",
    "_uuid": "712535409c4a19fb5d2bf34bc31459c137e9c7ae"
   },
   "source": [
    "Now, we'd like to to tokenize the titles. I use `split()` function. One could use `nltk.tokenize` as well. Based on **[this post](https://stackoverflow.com/questions/9602856/most-efficient-way-to-split-strings-in-python)**, `split()` works fairly good on not too long strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56e48d8b-3fac-44b9-895a-882699ccdb20",
    "_uuid": "c865aa39a040dd2593f718502664c15927e61aea",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizing the headlines\n",
    "train['tokenz'] = train['title_nopuncs'].apply(lambda x: x.split())\n",
    "train['tokenz'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c0428e50-9a66-467e-b958-ddbe3f5178ba",
    "_uuid": "a196290d2aca4fc32bf750f79b9f4bbf18cd35cd"
   },
   "source": [
    "Now, we should use find unique tokens. I can think of two approaches:\n",
    "* creates a master list of all the tokenz, and call unique() function on it.\n",
    "* create an emppty list, and append the unique tokenz to it. **Don't do this! It takes forever! Obviously.**\n",
    "\n",
    "\n",
    "**OR** use **[this](https://stackoverflow.com/questions/1720421/how-to-concatenate-two-lists-in-python)** awesome post on StackOverflow and find the following approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d140da2e-7f7b-4219-b38f-bc07b28cbcd2",
    "_uuid": "5e5ee50ffa9bae6a23d776710cd88784783d55e2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#this will create a list of all words\n",
    "words=list(itertools.chain.from_iterable(train['tokenz']))\n",
    "\n",
    "#this will create a list of unique words\n",
    "unique_words=list(set(words))\n",
    "\n",
    "print('Number of unique words:',len(set(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4210b0a0-0bb8-4081-9f40-9f5d88dfa03f",
    "_uuid": "2abd65cdbac35f691b8eae826218562ed13a6493"
   },
   "source": [
    "Next, we should create the **bag of words** matrix. It is a way of representing text data while performing machine learning. The three steps in this approach are:\n",
    "* tokenizing : we have already taken care of this!\n",
    "* counting: This is what we are about to do. Basically we count how many times those unique words occured in each headline, and format this information in a dataframe.\n",
    "* normalizing: we don't want too frequent, and once-in-a-lifetime words exist in our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6d303c50-9235-4e87-a33b-df444576e97e",
    "_uuid": "42cb1adf282d171c6633ead7a4ee0b8ddabb0c47",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#forming a dataframe of 0 values\n",
    "counts = pd.DataFrame(0,index=np.arange(train.shape[0]), columns=unique_words)\n",
    "#counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "736bc46c-3f95-423a-b78e-9b494828b342",
    "_uuid": "d1497f8695a4d4e2ec441e6608c7b7cd9709b47f",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#now counting the number of words in each headline and adding it to our dataframe\n",
    "for index, row in train.iterrows():\n",
    "    for token in row['tokenz']:\n",
    "        counts.iloc[index][token]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8bcf971f-327e-4052-82de-26b948f18b6e",
    "_uuid": "db460175aab8d208d29ca83eafc295a3ed92dee3"
   },
   "source": [
    "Interestingly, we could use the `sklearn.feature_extraction` that does all the steps that we have just implemented!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6fcb4eff-91b1-4e2c-aff7-7f8250a4262a",
    "_uuid": "b0f200ff0b131637005c48de469ff999f58e91cf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X=vectorizer.fit_transform(list(train['title']))\n",
    "counts=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "#print( vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14db1e95-65ae-432e-bc5a-dbba75a03873",
    "_uuid": "6bddbccd839114fb57b1a9733782e8de69090d5b"
   },
   "source": [
    "Too many columns. There are two types of features that will reduce regression accuracy:\n",
    "* The ones that occur only a few times. These will cause over fitting.\n",
    "* The ones that occur too many times, such as `a` and `and`. These are often called `stopwords`, and do not indicate any relationship with the upvotes.  \n",
    "Let's remove any word that occur fewer than 5 and more than 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "784151b6-0801-49c0-8a80-8fe79fac2c94",
    "_uuid": "841b4557de936ba472efb40770831dbe3252a33f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_sum=counts.sum()\n",
    "counts=counts.drop(count_sum[(count_sum>100) | (count_sum<5)].index,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "97a7cf25-5689-4421-991b-0e504449aa04",
    "_uuid": "7bb1ce135a517940dca0e3ddd1a4a106217ffddf"
   },
   "source": [
    "## <center> Model Fitting <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "06f34d5b-2a88-4416-a349-d8ebdc78c29a",
    "_uuid": "186f22083c9aebb181910656a452f3f5734366a1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spliting data into train and validation sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(counts,train['num_points'],train_size=0.8,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "80baab98-5406-4a24-a904-d1c784430f61",
    "_uuid": "c6a68e37a5bd8ee4d477e02837c61c7db920d267",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "pred=lr.predict(X_test)\n",
    "rmse=(mean_squared_error(pred,y_test))**0.5\n",
    "print('RMSE is: {0:.2f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7139f163-6480-4249-b7db-57614e4c4314",
    "_uuid": "b84c0d0464a6f5f20859dbcd48200009c78f8833"
   },
   "source": [
    "Which is pretty high! But please remember we are only using 5% of the data set. A larger data would drastically enhance model accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
